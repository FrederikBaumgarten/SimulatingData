@article{rajtmajerHowFailureFalsify2022,
  title = {How Failure to Falsify in High-Volume Science Contributes to the Replication Crisis},
  author = {Rajtmajer, Sarah M and Errington, Timothy M and Hillary, Frank G},
  editor = {Rodgers, Peter},
  year = {2022},
  month = aug,
  journal = {eLife},
  volume = {11},
  pages = {e78830},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.78830},
  urldate = {2023-12-09},
  abstract = {The number of scientific papers published every year continues to increase, but scientific knowledge is not progressing at the same rate. Here we argue that a greater emphasis on falsification -- the direct testing of strong hypotheses -- would lead to faster progress by allowing well-specified hypotheses to be eliminated. We describe an example from neuroscience where there has been little work to directly test two prominent but incompatible hypotheses related to traumatic brain injury. Based on this example, we discuss how building strong hypotheses and then setting out to falsify them can bring greater precision to the clinical neurosciences, and argue that this approach could be beneficial to all areas of science.},
  keywords = {data science,falsification,open science,replication,reproducibility,science forum},
  file = {/Users/frederik/Zotero/storage/XV4MGFED/Rajtmajer et al. - 2022 - How failure to falsify in high-volume science cont.pdf}
}

@article{erringtonInvestigatingReplicabilityPreclinical2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  urldate = {2023-12-09},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary -- the replication was either a success or a failure -- and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/Users/frederik/Zotero/storage/4T9UV9CS/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{bernerWhyHowWe2022,
  title = {Why and How We Should Join the Shift from Significance Testing to Estimation},
  author = {Berner, Daniel and Amrhein, Valentin},
  year = {2022},
  journal = {Journal of Evolutionary Biology},
  volume = {35},
  number = {6},
  pages = {777--787},
  issn = {1420-9101},
  doi = {10.1111/jeb.14009},
  urldate = {2023-12-09},
  abstract = {A paradigm shift away from null hypothesis significance testing seems in progress. Based on simulations, we illustrate some of the underlying motivations. First, p-values vary strongly from study to study, hence dichotomous inference using significance thresholds is usually unjustified. Second, `statistically significant' results have overestimated effect sizes, a bias declining with increasing statistical power. Third, `statistically non-significant' results have underestimated effect sizes, and this bias gets stronger with higher statistical power. Fourth, the tested statistical hypotheses usually lack biological justification and are often uninformative. Despite these problems, a screen of 48 papers from the 2020 volume of the Journal of Evolutionary Biology exemplifies that significance testing is still used almost universally in evolutionary biology. All screened studies tested default null hypotheses of zero effect with the default significance threshold of p = 0.05, none presented a pre-specified alternative hypothesis, pre-study power calculation and the probability of `false negatives' (beta error rate). The results sections of the papers presented 49 significance tests on average (median 23, range 0--390). Of 41 studies that contained verbal descriptions of a `statistically non-significant' result, 26 (63\%) falsely claimed the absence of an effect. We conclude that studies in ecology and evolutionary biology are mostly exploratory and descriptive. We should thus shift from claiming to `test' specific hypotheses statistically to describing and discussing many hypotheses (possible true effect sizes) that are most compatible with our data, given our statistical model. We already have the means for doing so, because we routinely present compatibility (`confidence') intervals covering these hypotheses.},
  copyright = {{\copyright} 2022 The Authors. Journal of Evolutionary Biology published by John Wiley \& Sons Ltd on behalf of European Society for Evolutionary Biology.},
  langid = {english},
  keywords = {compatibility interval,effect size,null hypothesis,p-value,scientific method,statistical inference},
  file = {/Users/frederik/Zotero/storage/VL92A3A9/Berner and Amrhein - 2022 - Why and how we should join the shift from signific.pdf;/Users/frederik/Zotero/storage/QWCI6EMS/jeb.html}
}

@article{amrheinRewritingResultsLanguage2022,
  title = {Rewriting Results in the Language of Compatibility},
  author = {Amrhein, Valentin and Greenland, Sander},
  year = {2022},
  month = jul,
  journal = {Trends in Ecology \& Evolution},
  volume = {37},
  number = {7},
  pages = {567--568},
  issn = {01695347},
  doi = {10.1016/j.tree.2022.02.001},
  urldate = {2023-12-09},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/2Y6Z6JMH/Amrhein and Greenland - 2022 - Rewriting results in the language of compatibility.pdf}
}

@article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  year = {2019},
  month = mar,
  journal = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-019-00857-9},
  urldate = {2023-12-09},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Research data,Research management},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Comment\\
Subject\_term: Research data, Research management},
  file = {/Users/frederik/Zotero/storage/VK2YTI8G/Amrhein et al. - 2019 - Scientists rise up against statistical significanc.pdf;/Users/frederik/Zotero/storage/CUKBANV5/d41586-019-00857-9.html}
}

@article{lokenMeasurementErrorReplication2017,
  title = {Measurement Error and the Replication Crisis},
  author = {Loken, Eric and Gelman, Andrew},
  year = {2017},
  month = feb,
  journal = {Science},
  volume = {355},
  number = {6325},
  pages = {584--585},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aal3618},
  urldate = {2023-12-12},
  file = {/Users/frederik/Zotero/storage/L67KMGQT/Loken and Gelman - 2017 - Measurement error and the replication crisis.pdf}
}

@article{halseyFickleValueGenerates2015,
  title = {The Fickle {{P}} Value Generates Irreproducible Results},
  author = {Halsey, Lewis G. and {Curran-Everett}, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
  year = {2015},
  month = mar,
  journal = {Nature Methods},
  volume = {12},
  number = {3},
  pages = {179--185},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3288},
  urldate = {2023-12-12},
  abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
  copyright = {2015 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Biological techniques,Education,Medical research,Statistical methods},
  file = {/Users/frederik/Zotero/storage/GYRP7PTE/Halsey et al. - 2015 - The fickle P value generates irreproducible result.pdf}
}

@article{davisWritingStatisticalMethods2023,
  title = {Writing Statistical Methods for Ecologists},
  author = {Davis, Amy J. and Kay, Shannon},
  year = {2023},
  journal = {Ecosphere},
  volume = {14},
  number = {5},
  pages = {e4539},
  issn = {2150-8925},
  doi = {10.1002/ecs2.4539},
  urldate = {2024-06-06},
  abstract = {The Methods section is a key component of any ecology research publication containing detailed information on how the data were collected and analyzed. However, descriptions of which statistical methods were used and how they were applied can substantially vary and may not provide enough information for the analyses to be reproducible. Computational and statistical programming advances have allowed ecological researchers without a strong statistical or mathematical background to access and use increasingly complex statistical methods. Thus, statistical methods are written by and need to be accessible to researchers across a range of quantitative expertise. Poorly written Methods sections can incorrectly inflate the strength of or call into question the results of an analysis. Although there are resources available, we have not found one that is specific to writing statistical methods, includes all the elements we discuss, and is targeted for ecologists. Here we provide guidelines for ecological researchers when writing statistical methods and review frequent errors made in Statistical Methods sections. We highlight some common dos and don'ts when writing Statistical Methods sections and present a simple checklist to help guide authors with their writing to ensure reproducibility. We illustrate the use of this guidance with two examples.},
  copyright = {Published 2023. This article is a U.S. Government work and is in the public domain in the USA. Ecosphere published by Wiley Periodicals LLC on behalf of The Ecological Society of America.},
  langid = {english},
  keywords = {ecological communication,quantitative ecology,scientific writing,statistical methods,statistics education,writing clarity},
  file = {/Users/frederik/Zotero/storage/NUV58BZT/Davis and Kay - 2023 - Writing statistical methods for ecologists.pdf;/Users/frederik/Zotero/storage/FBPA2FVH/ecs2.html}
}

@article{wassersteinMovingWorld052019,
  title = {Moving to a {{World Beyond}} ``p {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2024-06-07},
  file = {/Users/frederik/Zotero/storage/QGLY2U5D/Wasserstein et al. - 2019 - Moving to a World Beyond “p  0.05”.pdf}
}

@article{woolstonPsychologyJournalBans2015,
  title = {Psychology Journal Bans {{P}} Values},
  author = {Woolston, Chris},
  year = {2015},
  month = mar,
  journal = {Nature},
  volume = {519},
  number = {7541},
  pages = {9--9},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/519009f},
  urldate = {2024-06-11},
  abstract = {Test for reliability of results `too easy to pass', say editors.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Media,Publishing},
  file = {/Users/frederik/Zotero/storage/T6JTB5RQ/Woolston - 2015 - Psychology journal bans P values.pdf}
}

@article{leekStatisticsValuesAre2015,
  title = {Statistics: {{P}} Values Are Just the Tip of the Iceberg},
  shorttitle = {Statistics},
  author = {Leek, Jeffrey T. and Peng, Roger D.},
  year = {2015},
  month = apr,
  journal = {Nature},
  volume = {520},
  number = {7549},
  pages = {612--612},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/520612a},
  urldate = {2024-06-11},
  abstract = {Ridding science of shoddy statistics will require scrutiny of every step, not merely the last one, say Jeffrey T. Leek and Roger D. Peng.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Careers,Mathematics and computing,Research management},
  file = {/Users/frederik/Zotero/storage/JJGU3YJ7/Leek and Peng - 2015 - Statistics P values are just the tip of the icebe.pdf}
}

@article{leeAlternativesValueConfidence2016,
  title = {Alternatives to {{P}} Value: Confidence Interval and Effect Size},
  shorttitle = {Alternatives to {{P}} Value},
  author = {Lee, Dong Kyu},
  year = {2016},
  month = dec,
  journal = {Korean Journal of Anesthesiology},
  volume = {69},
  number = {6},
  pages = {555--562},
  issn = {2005-6419},
  doi = {10.4097/kjae.2016.69.6.555},
  abstract = {The previous articles of the Statistical Round in the Korean Journal of Anesthesiology posed a strong enquiry on the issue of null hypothesis significance testing (NHST). P values lie at the core of NHST and are used to classify all treatments into two groups: "has a significant effect" or "does not have a significant effect." NHST is frequently criticized for its misinterpretation of relationships and limitations in assessing practical importance. It has now provoked criticism for its limited use in merely separating treatments that "have a significant effect" from others that do not. Effect sizes and CIs expand the approach to statistical thinking. These attractive estimates facilitate authors and readers to discriminate between a multitude of treatment effects. Through this article, I have illustrated the concept and estimating principles of effect sizes and CIs.},
  langid = {english},
  pmcid = {PMC5133225},
  pmid = {27924194},
  keywords = {Confidence intervals,Effect sizes,P value},
  file = {/Users/frederik/Zotero/storage/HC9S8RDM/Lee - 2016 - Alternatives to P value confidence interval and e.pdf}
}

@inproceedings{puGardenForkingPaths2018a,
  title = {The {{Garden}} of {{Forking Paths}} in {{Visualization}}: {{A Design Space}} for {{Reliable Exploratory Visual Analytics}} : {{Position Paper}}},
  shorttitle = {The {{Garden}} of {{Forking Paths}} in {{Visualization}}},
  booktitle = {2018 {{IEEE Evaluation}} and {{Beyond}} - {{Methodological Approaches}} for {{Visualization}} ({{BELIV}})},
  author = {Pu, Xiaoying and Kay, Matthew},
  year = {2018},
  month = oct,
  pages = {37--45},
  doi = {10.1109/BELIV.2018.8634103},
  urldate = {2024-06-11},
  abstract = {Turkey emphasized decades ago that taking exploratory findings as confirmatory is ``destructively foolish''. We reframe recent conversations about the reliability of results from exploratory visual analytics - such as the multiple comparisons problem-in terms of Gelman and Loken's garden of forking paths to lay out a design space for addressing the forking paths problem in visual analytics. This design space encompasses existing approaches to address the forking paths problem (multiple comparison correction) as well as solutions that have not been applied to exploratory visual analytics (regularization). We also discuss how perceptual bias correction techniques may be used to correct biases induced in analysts' understanding of their data due to the forking paths problem, and outline how this problem can be cast as a threat to validity within Munzner's Nested Model of visualization design. Finally, we suggest paper review guidelines to encourage reviewers to consider the forking paths problem when evaluating future designs of visual analytics tools.},
  keywords = {Analytical models,Data models,Data visualization,Human-centered computing,Sociology,Task analysis,Tools,Visual analytics,Visualization,Visualization design and evaluation methods},
  file = {/Users/frederik/Zotero/storage/LTPIT77Z/Pu and Kay - 2018 - The Garden of Forking Paths in Visualization A De.pdf;/Users/frederik/Zotero/storage/LK3CU8BH/8634103.html}
}

@article{stefanBigLittleLies2023,
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D.},
  year = {2023},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2024-06-11},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,false-positive rate,p-curve,questionable research practices,Shiny app,significance,simulation},
  file = {/Users/frederik/Zotero/storage/NYMCXDGH/Stefan and Schönbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf}
}

@article{mcshaneAbandonStatisticalSignificance2019,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1527253},
  urldate = {2024-06-11},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm---and the p-value thresholds intrinsic to it---as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to ``ban'' p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  keywords = {Null hypothesis significance testing,p-Value,Replication,Sociology of science,Statistical significance},
  file = {/Users/frederik/Zotero/storage/6LT6YFUB/McShane et al. - 2019 - Abandon Statistical Significance.pdf}
}

@article{ioannidisWhyMostPublished2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2024-06-11},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  file = {/Users/frederik/Zotero/storage/T3THNU92/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@article{szucsWhenNullHypothesis2017,
  title = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}: {{A Reassessment}}},
  shorttitle = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = aug,
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  publisher = {Frontiers},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  urldate = {2024-06-11},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology and biomedical science in general. We review these shortcomings and suggest that, after about 60 years of negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out. Instead, we should encourage either more in-depth statistical training of more researchers and/or more widespread involvement of professional statisticians in all research.},
  langid = {english},
  keywords = {bayesian methods,False positive findings,Null hypothesis significance testing,replication crisis,Research Methodology},
  file = {/Users/frederik/Zotero/storage/QWMR4FXA/Szucs and Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{amrheinEarthFlat052017,
  title = {The Earth Is Flat (p {$>$} 0.05): Significance Thresholds and the Crisis of Unreplicable Research},
  shorttitle = {The Earth Is Flat (p {$>$} 0.05)},
  author = {Amrhein, Valentin and {Korner-Nievergelt}, Fr{\"a}nzi and Roth, Tobias},
  year = {2017},
  month = jul,
  journal = {PeerJ},
  volume = {5},
  pages = {e3544},
  publisher = {PeerJ Inc.},
  issn = {2167-8359},
  doi = {10.7717/peerj.3544},
  urldate = {2024-06-13},
  abstract = {The widespread use of `statistical significance' as a license for making a claim of a scientific finding leads to considerable distortion of the scientific process (according to the American Statistical Association). We review why degrading p-values into `significant' and `nonsignificant' contributes to making studies irreproducible, or to making them seem irreproducible. A major problem is that we tend to take small p-values at face value, but mistrust results with larger p-values. In either case, p-values tell little about reliability of research, because they are hardly replicable even if an alternative hypothesis is true. Also significance (p {$\leq$} 0.05) is hardly replicable: at a good statistical power of 80\%, two studies will be `conflicting', meaning that one is significant and the other is not, in one third of the cases if there is a true effect. A replication can therefore not be interpreted as having failed only because it is nonsignificant. Many apparent replication failures may thus reflect faulty judgment based on significance thresholds rather than a crisis of unreplicable research. Reliable conclusions on replicability and practical importance of a finding can only be drawn using cumulative evidence from multiple independent studies. However, applying significance thresholds makes cumulative knowledge unreliable. One reason is that with anything but ideal statistical power, significant effect sizes will be biased upwards. Interpreting inflated significant results while ignoring nonsignificant results will thus lead to wrong conclusions. But current incentives to hunt for significance lead to selective reporting and to publication bias against nonsignificant findings. Data dredging, p-hacking, and publication bias should be addressed by removing fixed significance thresholds. Consistent with the recommendations of the late Ronald Fisher, p-values should be interpreted as graded measures of the strength of evidence against the null hypothesis. Also larger p-values offer some evidence against the null hypothesis, and they cannot be interpreted as supporting the null hypothesis, falsely concluding that `there is no effect'. Information on possible true effect sizes that are compatible with the data must be obtained from the point estimate, e.g., from a sample average, and from the interval estimate, such as a confidence interval. We review how confusion about interpretation of larger p-values can be traced back to historical disputes among the founders of modern statistics. We further discuss potential arguments against removing significance thresholds, for example that decision rules should rather be more stringent, that sample sizes could decrease, or that p-values should better be completely abandoned. We conclude that whatever method of statistical inference we use, dichotomous threshold thinking must give way to non-automated informed judgment.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/6TYCPIGH/Amrhein et al. - 2017 - The earth is flat (p  0.05) significance thresho.pdf}
}

@article{heldReplicationPowerRegression2020,
  title = {Replication {{Power}} and {{Regression}} to {{The Mean}}},
  author = {Held, Leonhard and Pawel, Samuel and Schwab, Simon},
  year = {2020},
  month = dec,
  journal = {Significance},
  volume = {17},
  number = {6},
  pages = {10--11},
  issn = {1740-9705},
  doi = {10.1111/1740-9713.01462},
  urldate = {2024-06-13},
  abstract = {If a scientific study reports a discovery with a p-value at or around 0.05, how credible is it? And what are the chances that a replication of this study will produce a similarly ``significant'' finding? Leonhard Held, Samuel Pawel and Simon Schwab's answers may surprise you},
  file = {/Users/frederik/Zotero/storage/QKA29EGG/Held et al. - 2020 - Replication Power and Regression to The Mean.pdf;/Users/frederik/Zotero/storage/7BWCHNMK/7038140.html}
}

@article{schwabTenSimpleRules2022a,
  title = {Ten Simple Rules for Good Research Practice},
  author = {Schwab, Simon and Janiaud, Perrine and Dayan, Michael and Amrhein, Valentin and Panczak, Radoslaw and Palagi, Patricia M. and Hemkens, Lars G. and Ramon, Meike and Rothen, Nicolas and Senn, Stephen and Furrer, Eva and Held, Leonhard},
  year = {2022},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {6},
  pages = {e1010139},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010139},
  urldate = {2024-06-13},
  langid = {english},
  keywords = {Clinical trials,Data management,Publication ethics,Reproducibility,Research design,Research reporting guidelines,Statistical data,Systematic reviews},
  file = {/Users/frederik/Zotero/storage/JILYDFFQ/Schwab et al. - 2022 - Ten simple rules for good research practice.pdf}
}

@article{tverskyJudgmentUncertaintyHeuristics1974,
  title = {Judgment under {{Uncertainty}}: {{Heuristics}} and {{Biases}}},
  shorttitle = {Judgment under {{Uncertainty}}},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1974},
  month = sep,
  journal = {Science},
  volume = {185},
  number = {4157},
  pages = {1124--1131},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.185.4157.1124},
  urldate = {2024-07-06},
  abstract = {This article described three heuristics that are employed in making judgments under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgments and decisions in situations of uncertainty.},
  file = {/Users/frederik/Zotero/storage/HBIIESAY/Tversky and Kahneman - 1974 - Judgment under Uncertainty Heuristics and Biases.pdf}
}

@article{nickersonConfirmationBiasUbiquitous1998,
  title = {Confirmation {{Bias}}: {{A Ubiquitous Phenomenon}} in {{Many Guises}}},
  shorttitle = {Confirmation {{Bias}}},
  author = {Nickerson, Raymond S.},
  year = {1998},
  month = jun,
  journal = {Review of General Psychology},
  volume = {2},
  number = {2},
  pages = {175--220},
  publisher = {SAGE Publications Inc},
  issn = {1089-2680},
  doi = {10.1037/1089-2680.2.2.175},
  urldate = {2024-07-06},
  abstract = {Confirmation bias, as the term is typically used in the psychological literature, connotes the seeking or interpreting of evidence in ways that are partial to existing beliefs, expectations, or a hypothesis in hand. The author reviews evidence of such a bias in a variety of guises and gives examples of its operation in several practical contexts. Possible explanations are considered, and the question of its utility or disutility is discussed.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/QM2X3BEX/Nickerson - 1998 - Confirmation Bias A Ubiquitous Phenomenon in Many.pdf}
}

@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2024-08-19},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1--15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516--36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Economics,Psychology},
  file = {/Users/frederik/Zotero/storage/7G4IUECX/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{goodmanAligningStatisticalScientific2016,
  title = {Aligning Statistical and Scientific Reasoning},
  author = {Goodman, Steven N.},
  year = {2016},
  month = jun,
  journal = {Science},
  volume = {352},
  number = {6290},
  pages = {1180--1181},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaf5406},
  urldate = {2024-08-19},
  file = {/Users/frederik/Zotero/storage/795GD7BR/Goodman - 2016 - Aligning statistical and scientific reasoning.pdf}
}

@article{pearlSevenToolsCausal2019,
  title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
  author = {Pearl, Judea},
  year = {2019},
  month = feb,
  journal = {Commun. ACM},
  volume = {62},
  number = {3},
  pages = {54--60},
  issn = {0001-0782},
  doi = {10.1145/3241036},
  urldate = {2024-08-20},
  abstract = {The kind of causal inference seen in natural human thought can be "algorithmitized" to help produce human-level machine intelligence.},
  file = {/Users/frederik/Zotero/storage/THFU9AEK/Pearl - 2019 - The seven tools of causal inference, with reflecti.pdf}
}

@techreport{amrheinAbandoningStatisticalSignificance2019,
  title = {Abandoning Statistical Significance Is Both Sensible and Practical},
  author = {Amrhein, Valentin and Gelman, Andrew and Greenland, Sander and McShane, Blakeley B.},
  year = {2019},
  month = apr,
  number = {e27657v1},
  institution = {PeerJ Inc.},
  issn = {2167-9843},
  doi = {10.7287/peerj.preprints.27657v1},
  urldate = {2024-08-20},
  abstract = {To the Editor of JAMA Dr Ioannidis writes against our proposals to abandon statistical significance in scientific reasoning and publication, as endorsed in the editorial of a recent special issue of an American Statistical Association journal devoted to moving to a ``post p{$<$}0.05 world.'' We appreciate that he echoes our calls for ``embracing uncertainty, avoiding hyped claims{\dots}and recognizing `statistical significance' is often poorly understood.'' We also welcome his agreement that the ``interpretation of any result is far more complicated than just significance testing'' and that ``clinical, monetary, and other considerations may often have more importance than statistical findings.'' Nonetheless, we disagree that a statistical significance-based ``filtering process is useful to avoid drowning in noise'' in science and instead view such filtering as harmful. First, the implicit rule to not publish nonsignificant results biases the literature with overestimated effect sizes and encourages ``hacking'' to get significance. Second, nonsignificant results are often wrongly treated as zero. Third, significant results are often wrongly treated as truth rather than as the noisy estimates they are, thereby creating unrealistic expectations of replicability. Fourth, filtering on statistical significance provides no guarantee against noise. Instead, it amplifies noise because the quantity on which the filtering is based (the p-value) is itself extremely noisy and is made more so by dichotomizing it. We also disagree that abandoning statistical significance will reduce science to ``a state of statistical anarchy.'' Indeed, the journal Epidemiology banned statistical significance in 1990 and is today recognized as a leader in the field. Valid synthesis requires accounting for all relevant evidence---not just the subset that attained statistical significance. Thus, researchers should report more, not less, providing estimates and uncertainty statements for all quantities, justifying any exceptions, and considering ways the results are wrong. Publication criteria should be based on evaluating study design, data quality, and scientific content---not statistical significance. Decisions are seldom necessary in scientific reporting. However, when they are required (as in clinical practice), they should be made based on the costs, benefits, and likelihoods of all possible outcomes, not via arbitrary cutoffs applied to statistical summaries such as p-values which capture little of this picture. The replication crisis in science is not the product of the publication of unreliable findings. The publication of unreliable findings is unavoidable: as the saying goes, if we knew what we were doing, it would not be called research. Rather, the replication crisis has arisen because unreliable findings are presented as reliable.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/3BIYI35A/Amrhein et al. - 2019 - Abandoning statistical significance is both sensib.pdf}
}

@article{vanzwetSignificanceFilterWinner2021,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink},
  author = {Van Zwet, Erik W. and Cator, Eric A.},
  year = {2021},
  month = nov,
  journal = {Statistica Neerlandica},
  volume = {75},
  number = {4},
  pages = {437--452},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/stan.12241},
  urldate = {2024-08-20},
  abstract = {The ``significance filter'' refers to focusing exclusively on statistically significant results. Since frequentist properties such as unbiasedness and coverage are valid only before the data have been observed, there are no guarantees if we condition on significance. In fact, the significance filter leads to overestimation of the magnitude of the parameter, which has been called the ``winner's curse.'' It can also lead to undercoverage of the confidence interval. Moreover, these problems become more severe if the power is low. These issues clearly deserve our attention. They have been studied mostly through empirical observation and simulation, while there are relatively few mathematical results. Here we study them both from the frequentist and the Bayesian perspective. We prove that the relative bias of the magnitude is a decreasing function of the power and that the usual confidence interval undercovers when the power is less than 50\%. We conclude that it is important to apply the appropriate amount of shrinkage to counter the winner's curse.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/IRUQHJSI/Van Zwet and Cator - 2021 - The significance filter, the winner's curse and th.pdf}
}

@article{gelmanGardenForkingPaths2013,
  title = {The Garden of Forking Paths: {{Why}} Multiple Comparisons Can Be a Problem, Even When There Is No ``Fishing Expedition'' or ``p-Hacking'' and the Research Hypothesis Was Posited Ahead of Time},
  shorttitle = {The Garden of Forking Paths},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2013},
  journal = {Department of Statistics, Columbia University},
  volume = {348},
  number = {1-17},
  pages = {3},
  urldate = {2024-08-20},
  file = {/Users/frederik/Zotero/storage/4UK5H5SE/Gelman and Loken - 2013 - The garden of forking paths Why multiple comparis.pdf}
}

@misc{wolkovichFourstepBayesianWorkflow2024a,
  title = {A Four-Step {{Bayesian}} Workflow for Improving Ecological Science},
  author = {Wolkovich, E. M. and Davies, T. Jonathan and Pearse, William D. and Betancourt, Michael},
  year = {2024},
  month = aug,
  number = {arXiv:2408.02603},
  eprint = {2408.02603},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.02603},
  urldate = {2024-08-20},
  abstract = {Growing anthropogenic pressures have increased the need for robust predictive models. Meeting this demand requires approaches that can handle bigger data to yield forecasts that capture the variability and underlying uncertainty of ecological systems. Bayesian models are especially adept at this and are growing in use in ecology. Yet many ecologists today are not trained to take advantage of the bigger ecological data needed to generate more flexible robust models. Here we describe a broadly generalizable workflow for statistical analyses and show how it can enhance training in ecology. Building on the increasingly computational toolkit of many ecologists, this approach leverages simulation to integrate model building and testing for empirical data more fully with ecological theory. In turn this workflow can fit models that are more robust and well-suited to provide new ecological insights -- allowing us to refine where to put resources for better estimates, better models, and better forecasts.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Quantitative Methods},
  file = {/Users/frederik/Zotero/storage/MFUFHH46/Wolkovich et al. - 2024 - A four-step Bayesian workflow for improving ecolog.pdf;/Users/frederik/Zotero/storage/GWFG6ER3/2408.html}
}

@article{bornmannGrowthRatesModern2015,
  title = {Growth Rates of Modern Science: {{A}} Bibliometric Analysis Based on the Number of Publications and Cited References},
  shorttitle = {Growth Rates of Modern Science},
  author = {Bornmann, Lutz and Mutz, R{\"u}diger},
  year = {2015},
  journal = {Journal of the Association for Information Science and Technology},
  volume = {66},
  number = {11},
  pages = {2215--2222},
  issn = {2330-1643},
  doi = {10.1002/asi.23329},
  urldate = {2024-08-21},
  abstract = {Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique---segmented regression analysis---which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid-1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1\% up to the middle of the 18th century, to 2 to 3\% up to the period between the two world wars, and 8 to 9\% to 2010.},
  copyright = {{\copyright} 2015 ASIS\&T},
  langid = {english},
  keywords = {bibliometrics},
  file = {/Users/frederik/Zotero/storage/5732FK6H/Bornmann and Mutz - 2015 - Growth rates of modern science A bibliometric ana.pdf;/Users/frederik/Zotero/storage/4ZEVLLTP/asi.html}
}

@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  urldate = {2024-08-22},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Peer review,Publishing,Research management},
  file = {/Users/frederik/Zotero/storage/RAW48H5U/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;/Users/frederik/Zotero/storage/IVFDX65N/533452a.html}
}

@article{munafoManifestoReproducibleScience2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  urldate = {2024-08-22},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  copyright = {2017 Macmillan Publishers Limited},
  langid = {english},
  keywords = {Social sciences},
  file = {/Users/frederik/Zotero/storage/BGBS4QAA/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}

@article{linQuantifyingPublicationBias2018,
  title = {Quantifying {{Publication Bias}} in {{Meta-Analysis}}},
  author = {Lin, Lifeng and Chu, Haitao},
  year = {2018},
  month = sep,
  journal = {Biometrics},
  volume = {74},
  number = {3},
  pages = {785--794},
  issn = {0006-341X},
  doi = {10.1111/biom.12817},
  urldate = {2024-08-27},
  abstract = {Publication bias is a serious problem in systematic reviews and meta-analyses, which can affect the validity and generalization of conclusions. Currently, approaches to dealing with publication bias can be distinguished into two classes: selection models and funnel-plot-based methods. Selection models use weight functions to adjust the overall effect size estimate and are usually employed as sensitivity analyses to assess the potential impact of publication bias. Funnel-plot-based methods include visual examination of a funnel plot, regression and rank tests, and the nonparametric trim and fill method. Although these approaches have been widely used in applications, measures for quantifying publication bias are seldom studied in the literature. Such measures can be used as a characteristic of a meta-analysis; also, they permit comparisons of publication biases between different meta-analyses. Egger's regression intercept may be considered as a candidate measure, but it lacks an intuitive interpretation. This article introduces a new measure, the skewness of the standardized deviates, to quantify publication bias. This measure describes the asymmetry of the collected studies' distribution. In addition, a new test for publication bias is derived based on the skewness. Large sample properties of the new measure are studied, and its performance is illustrated using simulations and three case studies.},
  file = {/Users/frederik/Zotero/storage/85QWA7WY/Lin and Chu - 2018 - Quantifying Publication Bias in Meta-Analysis.pdf;/Users/frederik/Zotero/storage/ZFZKWNWT/7525830.html}
}

@article{yangPublicationBiasImpacts2023,
  title = {Publication Bias Impacts on Effect Size, Statistical Power, and Magnitude ({{Type M}}) and Sign ({{Type S}}) Errors in Ecology and Evolutionary Biology},
  author = {Yang, Yefeng and {S{\'a}nchez-T{\'o}jar}, Alfredo and O'Dea, Rose E. and Noble, Daniel W. A. and Koricheva, Julia and Jennions, Michael D. and Parker, Timothy H. and Lagisz, Malgorzata and Nakagawa, Shinichi},
  year = {2023},
  month = apr,
  journal = {BMC Biology},
  volume = {21},
  number = {1},
  pages = {71},
  issn = {1741-7007},
  doi = {10.1186/s12915-022-01485-y},
  urldate = {2024-08-27},
  abstract = {Collaborative efforts to directly replicate empirical studies in the medical and social sciences have revealed alarmingly low rates of replicability, a phenomenon dubbed the `replication crisis'. Poor replicability has spurred cultural changes targeted at improving reliability in these disciplines. Given the absence of equivalent replication projects in ecology and evolutionary biology, two inter-related indicators offer the opportunity to retrospectively assess replicability: publication bias and statistical power. This registered report assesses the prevalence and severity of small-study (i.e., smaller studies reporting larger effect sizes) and decline effects (i.e., effect sizes decreasing over time) across ecology and evolutionary biology using 87 meta-analyses comprising 4,250 primary studies and 17,638 effect sizes. Further, we estimate how publication bias might distort the estimation of effect sizes, statistical power, and errors in magnitude (Type M or exaggeration ratio) and sign (Type S). We show strong evidence for the pervasiveness of both small-study and decline effects in ecology and evolution. There was widespread prevalence of publication bias that resulted in meta-analytic means being over-estimated by (at least) 0.12 standard deviations. The prevalence of publication bias distorted confidence in meta-analytic results, with 66\% of initially statistically significant meta-analytic means becoming non-significant after correcting for publication bias. Ecological and evolutionary studies consistently had low statistical power (15\%) with a 4-fold exaggeration of effects on average (Type M error rates = 4.4). Notably, publication bias reduced power from 23\% to 15\% and increased type M error rates from 2.7 to 4.4 because it creates a non-random sample of effect size evidence. The sign errors of effect sizes (Type S error) increased from 5\% to 8\% because of publication bias. Our research provides clear evidence that many published ecological and evolutionary findings are inflated. Our results highlight the importance of designing high-power empirical studies (e.g., via collaborative team science), promoting and encouraging replication studies, testing and correcting for publication bias in meta-analyses, and adopting open and transparent research practices, such as (pre)registration, data- and code-sharing, and transparent reporting.},
  langid = {english},
  keywords = {Generalizability,Many labs,Meta-research,Open science,P-hacking,Questionable research practices,Registered report,Replicability,Reproducibility,Selective reporting,Transparency},
  file = {/Users/frederik/Zotero/storage/Q7R9PKZ9/Yang et al. - 2023 - Publication bias impacts on effect size, statistic.pdf}
}

@article{gelmanPowerCalculationsAssessing2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  urldate = {2024-08-27},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/9D35GQGC/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign).pdf}
}

@article{rosenthalFileDrawerProblem1979a,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological bulletin},
  volume = {86},
  number = {3},
  pages = {638},
  publisher = {American Psychological Association},
  urldate = {2024-08-27},
  file = {/Users/frederik/Zotero/storage/32BFJC3R/Rosenthal - 1979 - The file drawer problem and tolerance for null res.pdf}
}

@article{rubinDoesPreregistrationImprove2020,
  title = {Does Preregistration Improve the Credibility of Research Findings?},
  author = {Rubin, Mark},
  year = {2020},
  month = may,
  journal = {The Quantitative Methods for Psychology},
  volume = {16},
  number = {4},
  eprint = {2010.10513},
  primaryclass = {stat},
  pages = {376--390},
  issn = {2292-1354},
  doi = {10.20982/tqmp.16.4.p376},
  urldate = {2024-08-27},
  abstract = {Preregistration entails researchers registering their planned research hypotheses, methods, and analyses in a time-stamped document before they undertake their data collection and analyses. This document is then made available with the published research report to allow readers to identify discrepancies between what the researchers originally planned to do and what they actually ended up doing. This historical transparency is supposed to facilitate judgments about the credibility of the research findings. The present article provides a critical review of 17 of the reasons behind this argument. The article covers issues such as HARKing, multiple testing, p-hacking, forking paths, optional stopping, researchers' biases, selective reporting, test severity, publication bias, and replication rates. It is concluded that preregistration's historical transparency does not facilitate judgments about the credibility of research findings when researchers provide contemporary transparency in the form of (a) clear rationales for current hypotheses and analytical approaches, (b) public access to research data, materials, and code, and (c) demonstrations of the robustness of research conclusions to alternative interpretations and analytical approaches.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Other Statistics},
  file = {/Users/frederik/Zotero/storage/J5FQRQEA/Rubin - 2020 - Does preregistration improve the credibility of re.pdf;/Users/frederik/Zotero/storage/Z8N3JQ97/2010.html}
}

@article{wolkovichSimpleExplanationDeclining2021a,
  title = {A Simple Explanation for Declining Temperature Sensitivity with Warming},
  author = {Wolkovich, E. M. and Auerbach, J. and Chamberlain, C. J. and Buonaiuto, D. M. and Ettinger, A. K. and {Morales-Castilla}, I. and Gelman, A.},
  year = {2021},
  journal = {Global Change Biology},
  volume = {27},
  number = {20},
  pages = {4947--4949},
  issn = {1365-2486},
  doi = {10.1111/gcb.15746},
  urldate = {2024-08-27},
  abstract = {Recently, multiple studies have reported declining phenological sensitivities (∆ days per {\textcelsius}) with higher temperatures. Such observations have been used to suggest climate change is reshaping biological processes, with major implications for forecasts of future change. Here, we show that these results may simply be the outcome of using linear models to estimate nonlinear temperature responses, specifically for events that occur after a cumulative thermal threshold is met---a common model for many biological events. Corrections for the nonlinearity of temperature responses consistently remove the apparent decline. Our results show that rising temperatures combined with linear estimates based on calendar time produce the observations of declining sensitivity---without any shift in the underlying biology. Current methods may thus undermine efforts to identify when and how warming will reshape biological processes.},
  copyright = {{\copyright} 2021 John Wiley \& Sons Ltd},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/S8NPBBW5/Wolkovich et al. - 2021 - A simple explanation for declining temperature sen.pdf;/Users/frederik/Zotero/storage/MYHKD5GE/gcb.html}
}

@article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: {{Hypothesizing After}} the {{Results}} Are {{Known}}},
  shorttitle = {{{HARKing}}},
  author = {Kerr, Norbert L.},
  year = {1998},
  month = aug,
  journal = {Personality and Social Psychology Review},
  volume = {2},
  number = {3},
  pages = {196--217},
  publisher = {SAGE Publications Inc},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  urldate = {2024-08-27},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/VVHCCRAT/Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf}
}

@article{kokkoUsefulWaysBeing2005,
  title = {Useful Ways of Being Wrong},
  author = {Kokko, H.},
  year = {2005},
  month = sep,
  journal = {Journal of Evolutionary Biology},
  volume = {18},
  number = {5},
  pages = {1155--1157},
  issn = {1010-061X},
  doi = {10.1111/j.1420-9101.2004.00853.x},
  urldate = {2024-08-27},
  file = {/Users/frederik/Zotero/storage/8QP2SAY3/Kokko - 2005 - Useful ways of being wrong.pdf;/Users/frederik/Zotero/storage/J93I2ARN/7323737.html}
}
