\documentclass{article}

% required 
\usepackage[hyphens]{url} % this wraps my URL versus letting it spill across the page, a bad habit LaTeX has
\usepackage{Sweave}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{textcomp}%among other things, it allows degrees C to be added
\usepackage{float}
\usepackage[utf8]{inputenc} % allow funny letters in citations 
\usepackage[nottoc]{tocbibind} %should add Re fences to the table of contents?
\usepackage{amsmath} % making nice equations 
\usepackage{listings} % add in stan code
\usepackage{xcolor}
\usepackage{capt-of}%allows me to set a caption for code in appendix 
\usepackage[export]{adjustbox} % adding a box around a map
\usepackage{lineno}
\linenumbers
% recommended! Uncomment the below line and change the path for your computer!
% \SweaveOpts{prefix.string=/Users/Lizzie/Documents/git/teaching/demoSweave/Fig.s/demoFig, eps=FALSE} 
%put your Fig.s in one place! Also, note that here 'Fig.s' is the folder and 'demoFig' is what each 
% Fig. produced will be titled plus its number or label (e.g., demoFig-nqpbetter.pdf')
% make your captioning look better
\usepackage[small]{caption}

\usepackage{xr-hyper} %refer to Fig.s in another document
\usepackage{hyperref}

%\textbf{}\setlength{\captionmargin}{30pt}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt}

% optional: muck with spacing
\topmargin -1.5cm        
\oddsidemargin 0.5cm   
\evensidemargin 0.5cm  % same as odd side margin but for left-hand pages
\textwidth 15.59cm
\textheight 21.94cm 
% \renewcommand{\baselinestretch}{1.5} % 1.5 lines between lines
\parindent 0pt		  % sets leading space for paragraphs
% optional: cute, fancy headers
\usepackage{fancyhdr}
\pagestyle{fancy}
%\fancyhead[LO]{Frederik Baumgarten}
%\fancyhead[RO]{Research Proposal}
% more optionals! %

\graphicspath{ {/Users/frederik/github/SimulatingData/analyses/figures/} }% tell latex where to find figures 

\begin{document}
\renewcommand{\bibname}{References}%names reference list 


	\title{The power of simulating data: a tool to design experiments, understand data limitations and improve scientific reasoning \\%(my favourite)
	
	%Alternate title ideas:
	%2. Between noise and patterns: the power of data simulation in science to overcome perception biases\\
	%3. The power of simulating data: a tool for scientists from designing experiments to drawing/reaching reasonable conclusions\\
	%4. Between noise and patterns: Overcoming perception biases through data simulation
} 


\date{\today}
\author{Frederik Baumgarten\textsuperscript{1,2}, EM Wolkovich\textsuperscript{1}, invite Andrew Gelman}
\maketitle 

	$^1$ Department of Forest and Conservation, Faculty of Forestry, University of British Columbia, 2424 Main Mall
	Vancouver, BC Canada V6T 1Z4. \\
	
	$^2$  Swiss Federal Institute for Forest, Snow and Landscape Research WSL, Zürcherstr. 111, Birmensdorf 8903, Switzerland\\
	
Corresponding Author: Frederik Baumgarten; frederik.baumgarten@ubc.ca \\
Journal: statistical report in Ecology?



\section*{Abstract}
Science continually seeks to explain patterns in nature that are often obscured by the noise of variation caused by a myriad of influencing factors. To disentangle pattern from noise, link correlation with causality, and build upon the existing body of knowledge, researchers adhere to a scientific workflow. However, concerns have been raised about several aspects of this workflow: the formulation of meaningless (null) hypotheses, insufficient sample size, poor experimental design, and the problematic usage and interpretation of p-values. These issues have led to the reproducibility/replication crisis, biased reporting of findings, over-interpretation, or incorrect conclusions, with patterns sometimes emerging from mathematical artifacts or sheer chance.

Here, we propose the integration of data simulation into the scientific workflow to address these challenges by: 1) developing meaningful and testable hypotheses through the formulation of mathematical or mechanistic models, 2) manipulating effect size, variance, sample size and replication to generate synthetic/fake datasets, 3) exploring the potential and limitations of both the statistical methods and the data obtained, and 4) drawing conclusions that can reliably build upon the existing `body of knowledge'.

We expect that in a future dominated by powerful AI and machine learning algorithms applied to ever-larger datasets, meaningful, theoretically grounded hypotheses will be more important than ever for revealing underlying causalities. Without the understanding of causality, we may excel at making predictions within known limits, yet fail to understand the driving variables behind them.

\textbf{Keywords}: bayesian statistics, statistcal methods, quantitative ecology, hypothesis testing, scientific reasoning, experimental design, power analysis
\newpage

\section{Introduction}

\subsection*{The constant search for patterns}
A renowned researcher once explained to me the progression of a curve from a device attached to the bark of a tree next to us. As we watched the curve expand in real time on the screen, he gave a passionate and insightful explanation on the water relations of trees. A few moments later, however, the technician in charge entrusted me with the fact that the fluctuations on the graph emerged largely at random during the installation and calibration process and that the data displayed could not possibly represent anything biologically meaningful. This anecdote---which may sound familiar to many of us---illustrates 1) our tendency to interpret data in a way that supports our pre-existing beliefs, also known as confirmation bias \citep{nickersonConfirmationBiasUbiquitous1998}, and 2) our incredible ability to recognise patterns even when there are none, or in other words, our poor intuition to judge what randomly generated data may look like. Human evolution has equipped us with a hypersensitive pattern detector and a set of cognitive biases \citep{tverskyJudgmentUncertaintyHeuristics1974} that help us survive but get in the way of objective science. However, distinguishing between noise and pattern is perhaps the most important skill for any researcher. 

\subsection*{How can we ensure scientific standards?}
To overcome cognitive biases and objectify science, researchers usually follow a scientific workflow illustrated in Figure \ref{fig:fig_1xxx}) that traditionally starts with identifying a research question and formulating a hypothesis and ends with a conclusion based on the statistical outcome given the data obtained and the hypothesis at hand (see for example \citet{schwabTenSimpleRules2022a}). 

					\begin{figure}
					\centering
					\includegraphics[width=0.9\textwidth]{figure_1.pdf} 
					\caption{Schematic overview of the current scientific workflow (blue circle) with its related problems (red text) and how some of them can be anticipated with the help of data simulation (loop in yellow). The critical step is the translation of a verbal hypothesis into a mechanistic model expressed in mathematical terms. We believe that this 'extraround'  fosters a better understanding of underlying processes founded in theory, clear predictions that can be evaluated and overall better science. The questions summarized in the green box should be addressed in the simulation loop before continuing on the blue circle, e.g. prior to the start of real data gathering/exploration.}
					\label{fig:fig_1xxx}
					\end{figure}
					
However, there is growing evidence that following this workflow is not enough. Serious concerns have been raised in the last decades that jeopardize/question scientific integrity and credibility and may slow down scientific progress despite of a record breaking and still increasing publishing rate (REF). Or as \citet{ioannidisWhyMostPublished2005a} put it in his title: “Why most published research findings are false". Going through the traditional workflow we briefly outline some major concerns and pitfalls (see also Figure \ref{fig:fig_1xxx}).

\subsection*{Hypothesis and predictions}
Beyond the purely exploratory nature of a study, a hypothesis well-founded in current theory is essential for causal inference and reasoning \citep{rajtmajerHowFailureFalsify2022}. However, studies greatly differ in the usefulness of their stated hypotheses, which can range from non-existent to too vague, subjective, untestable, multidirectional, or lacking any direction. Additionally, the common practice of testing to falsify or reject a null hypothesis has been shown to be of limited utility in most cases. This is because null hypotheses often oversimplify complex relationships into a binary outcome and are frequently rejected due to the myriad of influencing factors contributing to data variability \citep{rajtmajerHowFailureFalsify2022}. Null hypotheses are particularly problematic when combined with the common practice of significance testing (NHST; null hypothesis significance testing), making them unsuitable as a cornerstone in scientific research. \citep{szucsWhenNullHypothesis2017}, see also the problems involved with p-values and `significance' below. 

\subsection*{Experimental design and sample size}




Many studies are under-sampled and lack statistical power. . More precisely, they lack statistical power to detect and are prone to find statistically significant results by chance when in fact there is none (\cite{halseyFickleValueGenerates2015} see below), which has led to a replication crisis in many disciplines with many studies not being able to reproduce previously published results  \citep{ioannidisWhyMostPublished2005a, baker500ScientistsLift2016, camererEvaluatingReplicabilitySocial2018}. 
interactions

\subsection*{Problematic use of p-values}

A widely recognized problem, despite its prime role in statistical inference arise from the usage of p-values \citep{amrheinScientistsRiseStatistical2019, amrheinEarthFlat052017, halseyFickleValueGenerates2015}. Many studies misinterpret `statistical significance' as direct proof for both relevancy and certainty (REF). Even if p-values are interpreted correctly the multitude of tests often performed in a single study can lead to a 'fishing strategy' also known as p-hacking, that increases the chance of getting a statistically significant result \citep{stefanBigLittleLies2023}. Finding a p < 0.05 during the process of exploring and analyzing data can occur largely unintentional. Most analysis can easily find hundreds of possible ways and combinations to find a potentially relevant pattern, e.g. by including/excluding variables or subgroups. \citet{gelmanGardenForkingPaths2013} illustrated this problem with the metaphor of `the garden of the forking paths' pointing at the importance of study pre-registration and the awareness of the choices we make during the analysis \citep{rubinDoesPreregistrationImprove2020, }.
Hypothesizing after the results are known (HARKing) is an additional pitfall that sometimes dangerously attaches causal reasoning to significant results \citep{kerr}. Finally, these mentioned problems feed into a reporting and publication bias \citep{yangPublicationBiasImpacts2023, linQuantifyingPublicationBias2018} that can substantially distort the literature as was shown for meta-analyses \citep{vanzwetSignificanceFilterWinner2021}. Overall, there is a trend to abandon p-values and to retire `statisitical significance' as an outdated concept \citep{amrheinAbandoningStatisticalSignificance2019, bernerWhyHowWe2022, mcshaneAbandonStatisticalSignificance2019, woolstonPsychologyJournalBans2015, wassersteinMovingWorld052019, leeAlternativesValueConfidence2016}. 



\subsection*{Mathematical artefacts}
In some cases the way data are analyzed or displayed create patterns, that arise from mathematical properties, sometimes hard to understand. A famous example is the regression to the mean that is responsible for many reported effects that were later found to be overestimated or not present at all (REF, \cite{k}, dunninger kruger effect). 

Example of decreasing sensitivity from Lizzie? An example in recent ecology research 


\subsection*{Will AI solve these issues?}
The steep rise in machine learning has propelled recent advancements of artificial intelligence (AI). Current applications excell human-level intelligence by far in many aspects (REF) offering new opportunities for science REF. Many disciplines have already benefited from these very recent advancements, eg.... However, technologies to date are limited to pattern recognition based on correlations only \citep{pearlSevenToolsCausal2019}. The critical ingredient lacking is the ability of causal reasoning - a feature so far limited to humans  \citep{pearlSevenToolsCausal2019}. While it might be possible that we see further improvements towards logic and reasoning in AI systems the separation of causality from correlation will likely continue to play a pivotal role in science. Therefore 

problem with hypothesis remain. searching for a model so it includes much of the assumptions/hypotheses expected from a useful model\\


\subsection*{Aim: The three steps of data simulation}
%1. translation of verbal hypothesis into a mechanistic model
%2. power analysis: explore sample and effect size, variation etc
%3. model predictions and assumptions
Here, we propose a small addition/adjustment to the traditional scientific workflow: The integration of data simulation prior to data gathering or exploration (Figure \ref{fig:fig_1xxx}). Concretely, this involves the following three steps:
%1) the translation of a verbal/conceptual hypothesis into a mechanistic model, well-founded in theory, 2) the exploration of the potential and limit of the generated dataset (e.g. power analysis) and 3) prediction and model checks....\\

\subsection*{1. Translation of verbal hypothesis into a mechanistic model}
Models are hypothesis that we evaluate based on our data. Hence, multiple hypothesis testing means applying multiple models. To constrain and justify this process to a reasonable set of hypothesis we propose to build up mechanistic models by incorporating existing knowledge about the influencing variables and their relationships in question. Which explanatory variables are influencing y? Is it linear or non-linear? What do we know from the literature? This step nudges us to think carefully about our hypothesis in terms of model parameters---a great way to translate subjective and conceptual hypothesis to objective mathematical formulations.

\subsection*{2. Power analysis: Tweak the model and simulate}
We don't propose a common power analysis based on some significance threshold to calculate the probability of correctly rejecting a null hypothesis when it is false (1 - $\beta$). There is nothing wrong with that approach and we even think this a good start. However, here we propose to go further: Explore your model by simulating data from it. Is it doing what you think it is? Generate your fake dataset by manipulating sample size, effect size(s), errorterms and relationships within the mechanistic model to explore how your model behaves. This is a playful, yet powerful way to design an experiment and understanding what your model does. You will find out about the potentials and limits of your model. Besides assessing the statistical power we suggest to focus on m


We believe that incorporating these three steps are vital to do better i) greatly stimulate our mechanistic understanding of underlying processes, ii) facilitate to move away from p-values and significance testing and towards evaluating model parameters such as effect sizes and error intervals,  a vital tool to do better science and show how to do it\\
Helps to address current gaps/limitations:\\
build hypothesis, then formulation of mathematical model\\
better design experiments\\

In the following we go through some concrete examples and show how this procedure can look like. 


%\section{Bus example with simulation workflow}
%We all manage to go through this world with the help of models in our minds. Most of the time we are not aware of this fact but their implications are all around us. For instance when we wait for a bus we expect the waiting time to be lets say around 7±3 min or when we readjust our expected travel time after an incidence and add the variable 'traffic jam' to our internal model. In short, whenever we have an intuitive understanding of how the world works, our brain suggests basic models - mostly oversimplified and with increasing bias as soon as we enter non-linear relationships.
%- In many practical cases we get the chance to recalibrate our internal model (or lets call it intuition). For example when the bus after 10 min is still not there and we start realizing that we need to incorporate the variable 'traffic jam'.
%But in many cases we don't get this kind of feedback or at least not in an informative way because it is too complex to grasp for our minds. While daily life is arguably not a big drawback to this issue, in science it can be. \\

%\subsection*{Situation}
%model - show poisson distr.

%\subsection*{no what?}
%still waiting for the bus...outlayer?\\
%update the model - assumptions\\
%add variable traffic jam\\


\section*{Example 1:  Plant growth and nitrogen}
Data simulation is best shown in practice. The example presented here along with more complex ones can also be performed through the R script available in the supplement. \\
Every study starts with a research question. So lets start with this one: What is the influence of nitrogen fertilization on plant growth? We might want to perform an experiment on sunflowers and fertilize some with nitrogen to assess the biomass after one season. So our dependent variable y is biomass and we might have a clunky hypothesis that plants grow more when fertilized with nitrogen.

\subsection*{What influences y in what way?}
Here we can dive deep into the literature. What do we know that influences plant growth? Let's pick nitrogen concentration for simplicity to start with. Lets further assume there is a positive linear relationship of plant growth and nitrogen concentration. So we can express this relationship in a basic linear model with an intercept a (e.g. the biomass at zero nitrogen), a slope b (e.g. the increase of biomass per 1 unit increase in nitrogen) and an error term that captures the natural variability:

\begin{lstlisting}
	# Building up a mechanistic model
	y = a+b*x + error #model formula with x being nitrogen concentration
	error = rnorm(n,0,sigma) #error distribution 
\end{lstlisting}

Note that we chose an error distribution that is drawn from a normal distribution with mean 0 and a standard deviation of sigma.

\subsection*{Setting model parameters}
This is the where things get interesting and concrete: What do you think your model parameter should look like? In other words, what biomass do we expect under no nitrogen fertilization? What is the increase in biomass per unit increase in nitrogen? And how variable is this relationship, e.g. what is the background noise? Lets put some numbers here:
\begin{lstlisting}
	# specifying model parameters:
	a <- 30 #intercept
	b <- 7 #slope (effect size)
	sigma <- 5 #standard deviation of the error distribution.
\end{lstlisting}

\subsection*{Simulate!}
In order to simulate we only need to set sample size and create a meaningful range of x values, i.e. nitrogen concentrations:
\begin{lstlisting}
	n <- 50 #sample size
	x<-rnorm(n,10,4) #create data for nitrogen concentration
	y<-a+b*x + rnorm(n,0,sigma) #generate y values
	fake<-data.frame(x,y)
\end{lstlisting}


\subsection*{Fit your model and check the outcome}
Using your fake dataset you can fit your model, check if the model parameters are in the range you expect them to be and plot them. This is basically your hypothesis as concrete as it gets. 
\begin{lstlisting}
	fit_1<-stan_glm(y~x, data=fake) # fit the model
	print(fit_1, digits=2)
	plot(fake$x,fake$y, main="Data and fitted regression line")
	a_hat<-coef(fit_1)[1]
	b_hat<-coef(fit_1)[2]
	abline(a_hat, b_hat)
\end{lstlisting}

\section*{How to use this - Play!}
Going back and forward you use this tool to better understand what your model is doing. Is that a reasonable effect size? Is the sample size high enough to detect the effect with reasonable certainty? What if the background noise is much higher? You may realized quickly that this loop will give you confidence in your study design and analysis beyond a simple power analysis. 


\subsection*{Avoiding overconfidence}
play with replication while holding variance and effect size constant. p-value figure\\

\section*{Increasing importance in the future}

\subsection*{Avoiding overconfidence}
evergrowing Lit
with AI we must learn to ask better questions\\
the right questions and hypothesis that are testable with current + new methods. Build up house of knowledge instead of using new pattern finding algorithms\\
how to integrate with AI?\\

\section*{stuff I did't find place yet}

\cite{wolkovichFourstepBayesianWorkflow2024a}
	\newpage


\bibliography{Art_simulation}
\bibliographystyle{ecolett}






\end{document}
