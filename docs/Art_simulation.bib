@misc{AbandoningStatisticalSignificance,
  title = {Abandoning Statistical Significance Is Both Sensible and Practical [{{PeerJ Preprints}}]},
  urldate = {2023-12-09},
  howpublished = {https://peerj.com/preprints/27657/},
  file = {/Users/frederik/Zotero/storage/GQPKSFGC/Abandoning statistical significance is both sensib.pdf}
}

@article{amrheinRewritingResultsLanguage2022,
  title = {Rewriting Results in the Language of Compatibility},
  author = {Amrhein, Valentin and Greenland, Sander},
  year = {2022},
  month = jul,
  journal = {Trends in Ecology \& Evolution},
  volume = {37},
  number = {7},
  pages = {567--568},
  issn = {01695347},
  doi = {10.1016/j.tree.2022.02.001},
  urldate = {2023-12-09},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/2Y6Z6JMH/Amrhein and Greenland - 2022 - Rewriting results in the language of compatibility.pdf}
}

@article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  year = {2019},
  month = mar,
  journal = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-019-00857-9},
  urldate = {2023-12-09},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Research data,Research management},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Comment\\
Subject\_term: Research data, Research management},
  file = {/Users/frederik/Zotero/storage/VK2YTI8G/Amrhein et al. - 2019 - Scientists rise up against statistical significanc.pdf;/Users/frederik/Zotero/storage/CUKBANV5/d41586-019-00857-9.html}
}

@article{bernerWhyHowWe2022,
  title = {Why and How We Should Join the Shift from Significance Testing to Estimation},
  author = {Berner, Daniel and Amrhein, Valentin},
  year = {2022},
  journal = {Journal of Evolutionary Biology},
  volume = {35},
  number = {6},
  pages = {777--787},
  issn = {1420-9101},
  doi = {10.1111/jeb.14009},
  urldate = {2023-12-09},
  abstract = {A paradigm shift away from null hypothesis significance testing seems in progress. Based on simulations, we illustrate some of the underlying motivations. First, p-values vary strongly from study to study, hence dichotomous inference using significance thresholds is usually unjustified. Second, `statistically significant' results have overestimated effect sizes, a bias declining with increasing statistical power. Third, `statistically non-significant' results have underestimated effect sizes, and this bias gets stronger with higher statistical power. Fourth, the tested statistical hypotheses usually lack biological justification and are often uninformative. Despite these problems, a screen of 48 papers from the 2020 volume of the Journal of Evolutionary Biology exemplifies that significance testing is still used almost universally in evolutionary biology. All screened studies tested default null hypotheses of zero effect with the default significance threshold of p = 0.05, none presented a pre-specified alternative hypothesis, pre-study power calculation and the probability of `false negatives' (beta error rate). The results sections of the papers presented 49 significance tests on average (median 23, range 0--390). Of 41 studies that contained verbal descriptions of a `statistically non-significant' result, 26 (63\%) falsely claimed the absence of an effect. We conclude that studies in ecology and evolutionary biology are mostly exploratory and descriptive. We should thus shift from claiming to `test' specific hypotheses statistically to describing and discussing many hypotheses (possible true effect sizes) that are most compatible with our data, given our statistical model. We already have the means for doing so, because we routinely present compatibility (`confidence') intervals covering these hypotheses.},
  copyright = {{\copyright} 2022 The Authors. Journal of Evolutionary Biology published by John Wiley \& Sons Ltd on behalf of European Society for Evolutionary Biology.},
  langid = {english},
  keywords = {compatibility interval,effect size,null hypothesis,p-value,scientific method,statistical inference},
  file = {/Users/frederik/Zotero/storage/VL92A3A9/Berner and Amrhein - 2022 - Why and how we should join the shift from signific.pdf;/Users/frederik/Zotero/storage/QWCI6EMS/jeb.html}
}

@article{davisWritingStatisticalMethods2023,
  title = {Writing Statistical Methods for Ecologists},
  author = {Davis, Amy J. and Kay, Shannon},
  year = {2023},
  journal = {Ecosphere},
  volume = {14},
  number = {5},
  pages = {e4539},
  issn = {2150-8925},
  doi = {10.1002/ecs2.4539},
  urldate = {2024-06-06},
  abstract = {The Methods section is a key component of any ecology research publication containing detailed information on how the data were collected and analyzed. However, descriptions of which statistical methods were used and how they were applied can substantially vary and may not provide enough information for the analyses to be reproducible. Computational and statistical programming advances have allowed ecological researchers without a strong statistical or mathematical background to access and use increasingly complex statistical methods. Thus, statistical methods are written by and need to be accessible to researchers across a range of quantitative expertise. Poorly written Methods sections can incorrectly inflate the strength of or call into question the results of an analysis. Although there are resources available, we have not found one that is specific to writing statistical methods, includes all the elements we discuss, and is targeted for ecologists. Here we provide guidelines for ecological researchers when writing statistical methods and review frequent errors made in Statistical Methods sections. We highlight some common dos and don'ts when writing Statistical Methods sections and present a simple checklist to help guide authors with their writing to ensure reproducibility. We illustrate the use of this guidance with two examples.},
  copyright = {Published 2023. This article is a U.S. Government work and is in the public domain in the USA. Ecosphere published by Wiley Periodicals LLC on behalf of The Ecological Society of America.},
  langid = {english},
  keywords = {ecological communication,quantitative ecology,scientific writing,statistical methods,statistics education,writing clarity},
  file = {/Users/frederik/Zotero/storage/NUV58BZT/Davis and Kay - 2023 - Writing statistical methods for ecologists.pdf;/Users/frederik/Zotero/storage/FBPA2FVH/ecs2.html}
}

@misc{EarthFlat05,
  title = {The Earth Is Flat (p~{$>~$}0.05): Significance Thresholds and the Crisis of Unreplicable Research [{{PeerJ}}]},
  urldate = {2023-12-09},
  howpublished = {https://peerj.com/articles/3544/},
  file = {/Users/frederik/Zotero/storage/8HPBTHZZ/The earth is flat (p  0.05) significance thresho.pdf;/Users/frederik/Zotero/storage/QHGPNTEH/3544.html}
}

@article{erringtonInvestigatingReplicabilityPreclinical2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  urldate = {2023-12-09},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary -- the replication was either a success or a failure -- and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/Users/frederik/Zotero/storage/4T9UV9CS/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{halseyFickleValueGenerates2015,
  title = {The Fickle {{P}} Value Generates Irreproducible Results},
  author = {Halsey, Lewis G. and {Curran-Everett}, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
  year = {2015},
  month = mar,
  journal = {Nature Methods},
  volume = {12},
  number = {3},
  pages = {179--185},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3288},
  urldate = {2023-12-12},
  abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
  copyright = {2015 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Biological techniques,Education,Medical research,Statistical methods},
  file = {/Users/frederik/Zotero/storage/GYRP7PTE/Halsey et al. - 2015 - The fickle P value generates irreproducible result.pdf}
}

@article{lokenMeasurementErrorReplication2017,
  title = {Measurement Error and the Replication Crisis},
  author = {Loken, Eric and Gelman, Andrew},
  year = {2017},
  month = feb,
  journal = {Science},
  volume = {355},
  number = {6325},
  pages = {584--585},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aal3618},
  urldate = {2023-12-12},
  file = {/Users/frederik/Zotero/storage/L67KMGQT/Loken and Gelman - 2017 - Measurement error and the replication crisis.pdf}
}

@article{rajtmajerHowFailureFalsify2022,
  title = {How Failure to Falsify in High-Volume Science Contributes to the Replication Crisis},
  author = {Rajtmajer, Sarah M and Errington, Timothy M and Hillary, Frank G},
  editor = {Rodgers, Peter},
  year = {2022},
  month = aug,
  journal = {eLife},
  volume = {11},
  pages = {e78830},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.78830},
  urldate = {2023-12-09},
  abstract = {The number of scientific papers published every year continues to increase, but scientific knowledge is not progressing at the same rate. Here we argue that a greater emphasis on falsification -- the direct testing of strong hypotheses -- would lead to faster progress by allowing well-specified hypotheses to be eliminated. We describe an example from neuroscience where there has been little work to directly test two prominent but incompatible hypotheses related to traumatic brain injury. Based on this example, we discuss how building strong hypotheses and then setting out to falsify them can bring greater precision to the clinical neurosciences, and argue that this approach could be beneficial to all areas of science.},
  keywords = {data science,falsification,open science,replication,reproducibility,science forum},
  file = {/Users/frederik/Zotero/storage/XV4MGFED/Rajtmajer et al. - 2022 - How failure to falsify in high-volume science cont.pdf}
}

@misc{SignificanceFilterWinner,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink - {{Zwet}} - 2021 - {{Statistica Neerlandica}} - {{Wiley Online Library}}},
  urldate = {2023-12-09},
  howpublished = {https://onlinelibrary.wiley.com/doi/full/10.1111/stan.12241},
  file = {/Users/frederik/Zotero/storage/HS634MXS/stan.html}
}

@article{wassersteinMovingWorld052019,
  title = {Moving to a {{World Beyond}} ``p {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2024-06-07},
  file = {/Users/frederik/Zotero/storage/QGLY2U5D/Wasserstein et al. - 2019 - Moving to a World Beyond “p  0.05”.pdf}
}
