@misc{AbandoningStatisticalSignificance,
  title = {Abandoning Statistical Significance Is Both Sensible and Practical [{{PeerJ Preprints}}]},
  urldate = {2023-12-09},
  howpublished = {https://peerj.com/preprints/27657/},
  file = {/Users/frederik/Zotero/storage/GQPKSFGC/Abandoning statistical significance is both sensib.pdf}
}

@article{amrheinRewritingResultsLanguage2022,
  title = {Rewriting Results in the Language of Compatibility},
  author = {Amrhein, Valentin and Greenland, Sander},
  year = {2022},
  month = jul,
  journal = {Trends in Ecology \& Evolution},
  volume = {37},
  number = {7},
  pages = {567--568},
  issn = {01695347},
  doi = {10.1016/j.tree.2022.02.001},
  urldate = {2023-12-09},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/2Y6Z6JMH/Amrhein and Greenland - 2022 - Rewriting results in the language of compatibility.pdf}
}

@article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  year = {2019},
  month = mar,
  journal = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-019-00857-9},
  urldate = {2023-12-09},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Research data,Research management},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Comment\\
Subject\_term: Research data, Research management},
  file = {/Users/frederik/Zotero/storage/VK2YTI8G/Amrhein et al. - 2019 - Scientists rise up against statistical significanc.pdf;/Users/frederik/Zotero/storage/CUKBANV5/d41586-019-00857-9.html}
}

@article{bernerWhyHowWe2022,
  title = {Why and How We Should Join the Shift from Significance Testing to Estimation},
  author = {Berner, Daniel and Amrhein, Valentin},
  year = {2022},
  journal = {Journal of Evolutionary Biology},
  volume = {35},
  number = {6},
  pages = {777--787},
  issn = {1420-9101},
  doi = {10.1111/jeb.14009},
  urldate = {2023-12-09},
  abstract = {A paradigm shift away from null hypothesis significance testing seems in progress. Based on simulations, we illustrate some of the underlying motivations. First, p-values vary strongly from study to study, hence dichotomous inference using significance thresholds is usually unjustified. Second, `statistically significant' results have overestimated effect sizes, a bias declining with increasing statistical power. Third, `statistically non-significant' results have underestimated effect sizes, and this bias gets stronger with higher statistical power. Fourth, the tested statistical hypotheses usually lack biological justification and are often uninformative. Despite these problems, a screen of 48 papers from the 2020 volume of the Journal of Evolutionary Biology exemplifies that significance testing is still used almost universally in evolutionary biology. All screened studies tested default null hypotheses of zero effect with the default significance threshold of p = 0.05, none presented a pre-specified alternative hypothesis, pre-study power calculation and the probability of `false negatives' (beta error rate). The results sections of the papers presented 49 significance tests on average (median 23, range 0--390). Of 41 studies that contained verbal descriptions of a `statistically non-significant' result, 26 (63\%) falsely claimed the absence of an effect. We conclude that studies in ecology and evolutionary biology are mostly exploratory and descriptive. We should thus shift from claiming to `test' specific hypotheses statistically to describing and discussing many hypotheses (possible true effect sizes) that are most compatible with our data, given our statistical model. We already have the means for doing so, because we routinely present compatibility (`confidence') intervals covering these hypotheses.},
  copyright = {{\copyright} 2022 The Authors. Journal of Evolutionary Biology published by John Wiley \& Sons Ltd on behalf of European Society for Evolutionary Biology.},
  langid = {english},
  keywords = {compatibility interval,effect size,null hypothesis,p-value,scientific method,statistical inference},
  file = {/Users/frederik/Zotero/storage/VL92A3A9/Berner and Amrhein - 2022 - Why and how we should join the shift from signific.pdf;/Users/frederik/Zotero/storage/QWCI6EMS/jeb.html}
}

@article{davisWritingStatisticalMethods2023,
  title = {Writing Statistical Methods for Ecologists},
  author = {Davis, Amy J. and Kay, Shannon},
  year = {2023},
  journal = {Ecosphere},
  volume = {14},
  number = {5},
  pages = {e4539},
  issn = {2150-8925},
  doi = {10.1002/ecs2.4539},
  urldate = {2024-06-06},
  abstract = {The Methods section is a key component of any ecology research publication containing detailed information on how the data were collected and analyzed. However, descriptions of which statistical methods were used and how they were applied can substantially vary and may not provide enough information for the analyses to be reproducible. Computational and statistical programming advances have allowed ecological researchers without a strong statistical or mathematical background to access and use increasingly complex statistical methods. Thus, statistical methods are written by and need to be accessible to researchers across a range of quantitative expertise. Poorly written Methods sections can incorrectly inflate the strength of or call into question the results of an analysis. Although there are resources available, we have not found one that is specific to writing statistical methods, includes all the elements we discuss, and is targeted for ecologists. Here we provide guidelines for ecological researchers when writing statistical methods and review frequent errors made in Statistical Methods sections. We highlight some common dos and don'ts when writing Statistical Methods sections and present a simple checklist to help guide authors with their writing to ensure reproducibility. We illustrate the use of this guidance with two examples.},
  copyright = {Published 2023. This article is a U.S. Government work and is in the public domain in the USA. Ecosphere published by Wiley Periodicals LLC on behalf of The Ecological Society of America.},
  langid = {english},
  keywords = {ecological communication,quantitative ecology,scientific writing,statistical methods,statistics education,writing clarity},
  file = {/Users/frederik/Zotero/storage/NUV58BZT/Davis and Kay - 2023 - Writing statistical methods for ecologists.pdf;/Users/frederik/Zotero/storage/FBPA2FVH/ecs2.html}
}

@misc{EarthFlat05,
  title = {The Earth Is Flat (p~{$>~$}0.05): Significance Thresholds and the Crisis of Unreplicable Research [{{PeerJ}}]},
  urldate = {2023-12-09},
  howpublished = {https://peerj.com/articles/3544/},
  file = {/Users/frederik/Zotero/storage/8HPBTHZZ/The earth is flat (p  0.05) significance thresho.pdf;/Users/frederik/Zotero/storage/QHGPNTEH/3544.html}
}

@article{erringtonInvestigatingReplicabilityPreclinical2021,
  title = {Investigating the Replicability of Preclinical Cancer Biology},
  author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Pasqualini, Renata and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e71601},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.71601},
  urldate = {2023-12-09},
  abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary -- the replication was either a success or a failure -- and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
  keywords = {credibility,meta-analysis,replication,reproducibility,reproducibility in cancer biology,Reproducibility Project: Cancer Biology,transparency},
  file = {/Users/frederik/Zotero/storage/4T9UV9CS/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf}
}

@article{gelmanGardenForkingPaths,
  title = {The Garden of Forking Paths: {{Why}} Multiple Comparisons Can Be a Problem, Even When There Is No ``Fishing Expedition'' or ``p-Hacking'' and the Research Hypothesis Was Posited Ahead of Time},
  author = {Gelman, Andrew and Loken, Eric},
  abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data.},
  langid = {english},
  file = {/Users/frederik/Zotero/storage/6YK7UH5T/Gelman and Loken - The garden of forking paths Why multiple comparis.pdf}
}

@article{halseyFickleValueGenerates2015,
  title = {The Fickle {{P}} Value Generates Irreproducible Results},
  author = {Halsey, Lewis G. and {Curran-Everett}, Douglas and Vowler, Sarah L. and Drummond, Gordon B.},
  year = {2015},
  month = mar,
  journal = {Nature Methods},
  volume = {12},
  number = {3},
  pages = {179--185},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3288},
  urldate = {2023-12-12},
  abstract = {The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.},
  copyright = {2015 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Biological techniques,Education,Medical research,Statistical methods},
  file = {/Users/frederik/Zotero/storage/GYRP7PTE/Halsey et al. - 2015 - The fickle P value generates irreproducible result.pdf}
}

@article{ioannidisWhyMostPublished2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLOS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2024-06-11},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  langid = {english},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  file = {/Users/frederik/Zotero/storage/T3THNU92/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@article{leeAlternativesValueConfidence2016,
  title = {Alternatives to {{P}} Value: Confidence Interval and Effect Size},
  shorttitle = {Alternatives to {{P}} Value},
  author = {Lee, Dong Kyu},
  year = {2016},
  month = dec,
  journal = {Korean Journal of Anesthesiology},
  volume = {69},
  number = {6},
  pages = {555--562},
  issn = {2005-6419},
  doi = {10.4097/kjae.2016.69.6.555},
  abstract = {The previous articles of the Statistical Round in the Korean Journal of Anesthesiology posed a strong enquiry on the issue of null hypothesis significance testing (NHST). P values lie at the core of NHST and are used to classify all treatments into two groups: "has a significant effect" or "does not have a significant effect." NHST is frequently criticized for its misinterpretation of relationships and limitations in assessing practical importance. It has now provoked criticism for its limited use in merely separating treatments that "have a significant effect" from others that do not. Effect sizes and CIs expand the approach to statistical thinking. These attractive estimates facilitate authors and readers to discriminate between a multitude of treatment effects. Through this article, I have illustrated the concept and estimating principles of effect sizes and CIs.},
  langid = {english},
  pmcid = {PMC5133225},
  pmid = {27924194},
  keywords = {Confidence intervals,Effect sizes,P value},
  file = {/Users/frederik/Zotero/storage/HC9S8RDM/Lee - 2016 - Alternatives to P value confidence interval and e.pdf}
}

@article{leekStatisticsValuesAre2015,
  title = {Statistics: {{P}} Values Are Just the Tip of the Iceberg},
  shorttitle = {Statistics},
  author = {Leek, Jeffrey T. and Peng, Roger D.},
  year = {2015},
  month = apr,
  journal = {Nature},
  volume = {520},
  number = {7549},
  pages = {612--612},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/520612a},
  urldate = {2024-06-11},
  abstract = {Ridding science of shoddy statistics will require scrutiny of every step, not merely the last one, say Jeffrey T. Leek and Roger D. Peng.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Careers,Mathematics and computing,Research management},
  file = {/Users/frederik/Zotero/storage/JJGU3YJ7/Leek and Peng - 2015 - Statistics P values are just the tip of the icebe.pdf}
}

@article{lokenMeasurementErrorReplication2017,
  title = {Measurement Error and the Replication Crisis},
  author = {Loken, Eric and Gelman, Andrew},
  year = {2017},
  month = feb,
  journal = {Science},
  volume = {355},
  number = {6325},
  pages = {584--585},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aal3618},
  urldate = {2023-12-12},
  file = {/Users/frederik/Zotero/storage/L67KMGQT/Loken and Gelman - 2017 - Measurement error and the replication crisis.pdf}
}

@article{mcshaneAbandonStatisticalSignificance2019,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1527253},
  urldate = {2024-06-11},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm---and the p-value thresholds intrinsic to it---as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to ``ban'' p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  keywords = {Null hypothesis significance testing,p-Value,Replication,Sociology of science,Statistical significance},
  file = {/Users/frederik/Zotero/storage/6LT6YFUB/McShane et al. - 2019 - Abandon Statistical Significance.pdf}
}

@inproceedings{puGardenForkingPaths2018,
  title = {The {{Garden}} of {{Forking Paths}} in {{Visualization}}: {{A Design Space}} for {{Reliable Exploratory Visual Analytics}} : {{Position Paper}}},
  shorttitle = {The {{Garden}} of {{Forking Paths}} in {{Visualization}}},
  booktitle = {2018 {{IEEE Evaluation}} and {{Beyond}} - {{Methodological Approaches}} for {{Visualization}} ({{BELIV}})},
  author = {Pu, Xiaoying and Kay, Matthew},
  year = {2018},
  month = oct,
  pages = {37--45},
  doi = {10.1109/BELIV.2018.8634103},
  urldate = {2024-06-11},
  abstract = {Turkey emphasized decades ago that taking exploratory findings as confirmatory is ``destructively foolish''. We reframe recent conversations about the reliability of results from exploratory visual analytics - such as the multiple comparisons problem-in terms of Gelman and Loken's garden of forking paths to lay out a design space for addressing the forking paths problem in visual analytics. This design space encompasses existing approaches to address the forking paths problem (multiple comparison correction) as well as solutions that have not been applied to exploratory visual analytics (regularization). We also discuss how perceptual bias correction techniques may be used to correct biases induced in analysts' understanding of their data due to the forking paths problem, and outline how this problem can be cast as a threat to validity within Munzner's Nested Model of visualization design. Finally, we suggest paper review guidelines to encourage reviewers to consider the forking paths problem when evaluating future designs of visual analytics tools.},
  keywords = {Analytical models,Data models,Data visualization,Human-centered computing,Sociology,Task analysis,Tools,Visual analytics,Visualization,Visualization design and evaluation methods},
  file = {/Users/frederik/Zotero/storage/CLMVWMEM/Pu and Kay - 2018 - The Garden of Forking Paths in Visualization A De.pdf;/Users/frederik/Zotero/storage/Z7CZA37H/8634103.html}
}

@inproceedings{puGardenForkingPaths2018a,
  title = {The {{Garden}} of {{Forking Paths}} in {{Visualization}}: {{A Design Space}} for {{Reliable Exploratory Visual Analytics}} : {{Position Paper}}},
  shorttitle = {The {{Garden}} of {{Forking Paths}} in {{Visualization}}},
  booktitle = {2018 {{IEEE Evaluation}} and {{Beyond}} - {{Methodological Approaches}} for {{Visualization}} ({{BELIV}})},
  author = {Pu, Xiaoying and Kay, Matthew},
  year = {2018},
  month = oct,
  pages = {37--45},
  doi = {10.1109/BELIV.2018.8634103},
  urldate = {2024-06-11},
  abstract = {Turkey emphasized decades ago that taking exploratory findings as confirmatory is ``destructively foolish''. We reframe recent conversations about the reliability of results from exploratory visual analytics - such as the multiple comparisons problem-in terms of Gelman and Loken's garden of forking paths to lay out a design space for addressing the forking paths problem in visual analytics. This design space encompasses existing approaches to address the forking paths problem (multiple comparison correction) as well as solutions that have not been applied to exploratory visual analytics (regularization). We also discuss how perceptual bias correction techniques may be used to correct biases induced in analysts' understanding of their data due to the forking paths problem, and outline how this problem can be cast as a threat to validity within Munzner's Nested Model of visualization design. Finally, we suggest paper review guidelines to encourage reviewers to consider the forking paths problem when evaluating future designs of visual analytics tools.},
  keywords = {Analytical models,Data models,Data visualization,Human-centered computing,Sociology,Task analysis,Tools,Visual analytics,Visualization,Visualization design and evaluation methods},
  file = {/Users/frederik/Zotero/storage/LTPIT77Z/Pu and Kay - 2018 - The Garden of Forking Paths in Visualization A De.pdf;/Users/frederik/Zotero/storage/LK3CU8BH/8634103.html}
}

@article{rajtmajerHowFailureFalsify2022,
  title = {How Failure to Falsify in High-Volume Science Contributes to the Replication Crisis},
  author = {Rajtmajer, Sarah M and Errington, Timothy M and Hillary, Frank G},
  editor = {Rodgers, Peter},
  year = {2022},
  month = aug,
  journal = {eLife},
  volume = {11},
  pages = {e78830},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.78830},
  urldate = {2023-12-09},
  abstract = {The number of scientific papers published every year continues to increase, but scientific knowledge is not progressing at the same rate. Here we argue that a greater emphasis on falsification -- the direct testing of strong hypotheses -- would lead to faster progress by allowing well-specified hypotheses to be eliminated. We describe an example from neuroscience where there has been little work to directly test two prominent but incompatible hypotheses related to traumatic brain injury. Based on this example, we discuss how building strong hypotheses and then setting out to falsify them can bring greater precision to the clinical neurosciences, and argue that this approach could be beneficial to all areas of science.},
  keywords = {data science,falsification,open science,replication,reproducibility,science forum},
  file = {/Users/frederik/Zotero/storage/XV4MGFED/Rajtmajer et al. - 2022 - How failure to falsify in high-volume science cont.pdf}
}

@misc{SignificanceFilterWinner,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink - {{Zwet}} - 2021 - {{Statistica Neerlandica}} - {{Wiley Online Library}}},
  urldate = {2023-12-09},
  howpublished = {https://onlinelibrary.wiley.com/doi/full/10.1111/stan.12241},
  file = {/Users/frederik/Zotero/storage/HS634MXS/stan.html}
}

@article{stefanBigLittleLies2023,
  title = {Big Little Lies: A Compendium and Simulation of p-Hacking Strategies},
  shorttitle = {Big Little Lies},
  author = {Stefan, Angelika M. and Sch{\"o}nbrodt, Felix D.},
  year = {2023},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {2},
  pages = {220346},
  publisher = {Royal Society},
  doi = {10.1098/rsos.220346},
  urldate = {2024-06-11},
  abstract = {In many research fields, the widespread use of questionable research practices has jeopardized the credibility of scientific results. One of the most prominent questionable research practices is p-hacking. Typically, p-hacking is defined as a compound of strategies targeted at rendering non-significant hypothesis testing results significant. However, a comprehensive overview of these p-hacking strategies is missing, and current meta-scientific research often ignores the heterogeneity of strategies. Here, we compile a list of 12 p-hacking strategies based on an extensive literature review, identify factors that control their level of severity, and demonstrate their impact on false-positive rates using simulation studies. We also use our simulation results to evaluate several approaches that have been proposed to mitigate the influence of questionable research practices. Our results show that investigating p-hacking at the level of strategies can provide a better understanding of the process of p-hacking, as well as a broader basis for developing effective countermeasures. By making our analyses available through a Shiny app and R package, we facilitate future meta-scientific research aimed at investigating the ramifications of p-hacking across multiple strategies, and we hope to start a broader discussion about different manifestations of p-hacking in practice.},
  keywords = {error rates,false-positive rate,p-curve,questionable research practices,Shiny app,significance,simulation},
  file = {/Users/frederik/Zotero/storage/NYMCXDGH/Stefan and Schönbrodt - 2023 - Big little lies a compendium and simulation of p-.pdf}
}

@article{szucsWhenNullHypothesis2017,
  title = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}: {{A Reassessment}}},
  shorttitle = {When {{Null Hypothesis Significance Testing Is Unsuitable}} for {{Research}}},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = aug,
  journal = {Frontiers in Human Neuroscience},
  volume = {11},
  publisher = {Frontiers},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2017.00390},
  urldate = {2024-06-11},
  abstract = {Null hypothesis significance testing (NHST) has several shortcomings that are likely contributing factors behind the widely debated replication crisis of (cognitive) neuroscience, psychology and biomedical science in general. We review these shortcomings and suggest that, after about 60 years of negative experience, NHST should no longer be the default, dominant statistical practice of all biomedical and psychological research. If theoretical predictions are weak we should not rely on all or nothing hypothesis tests. Different inferential methods may be most suitable for different types of research questions. Whenever researchers use NHST they should justify its use, and publish pre-study power calculations and effect sizes, including negative findings. Hypothesis-testing studies should be pre-registered and optimally raw data published. The current statistics lite educational approach for students that has sustained the widespread, spurious use of NHST should be phased out. Instead, we should encourage either more in-depth statistical training of more researchers and/or more widespread involvement of professional statisticians in all research.},
  langid = {english},
  keywords = {bayesian methods,False positive findings,Null hypothesis significance testing,replication crisis,Research Methodology},
  file = {/Users/frederik/Zotero/storage/QWMR4FXA/Szucs and Ioannidis - 2017 - When Null Hypothesis Significance Testing Is Unsui.pdf}
}

@article{wassersteinMovingWorld052019,
  title = {Moving to a {{World Beyond}} ``p {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2024-06-07},
  file = {/Users/frederik/Zotero/storage/QGLY2U5D/Wasserstein et al. - 2019 - Moving to a World Beyond “p  0.05”.pdf}
}

@article{woolstonPsychologyJournalBans2015,
  title = {Psychology Journal Bans {{P}} Values},
  author = {Woolston, Chris},
  year = {2015},
  month = mar,
  journal = {Nature},
  volume = {519},
  number = {7541},
  pages = {9--9},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/519009f},
  urldate = {2024-06-11},
  abstract = {Test for reliability of results `too easy to pass', say editors.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Media,Publishing},
  file = {/Users/frederik/Zotero/storage/T6JTB5RQ/Woolston - 2015 - Psychology journal bans P values.pdf}
}
